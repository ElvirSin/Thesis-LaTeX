%--------------------------------------------------------------------------------------------------
\chapter{Umsetzung des Konzepts mit Hilfe der Unity Enginge}\label{cha:Umsetzung}
In diesem Kapitel wird die Umsetzung des Menschmodells und der Interaktionsschnittstelle mit Hilfe der Entwicklungsumgebung der Unity Engine vorgestellt. Daher gibt es zunächst einen Einblick in die genutzte Hardware und Software, bevor das Menschmodell und die Interaktionsschnittstelle genauer erläutert werden.
%--------------------------------------------------------------------------------------------------
\section{Eingesetzte Hardware}\label{sec:Hardware}
Für die Umsetzung dieses Projektes wurde die Virtual Reality Brille \textbf{VIVE Pro} (Vgl. Abbildung \ref{fig:ViveproKit}) vom Hersteller HTC verwendet, da sie eine der leistungsstärksten Brillen auf dem Markt ist. Zu den Stärken dieser Brille gehören das kontrastreiche OLED-Display, die sehr hohe Auflösung von 1440 x 1600 Pixeln pro Auge, eine Bildwiederholrate von 90Hz, ein Sichtfeld von 110 Grad und vor allem die Möglichkeit die Brille mit Hilfe des \textbf{Vive Wireless Sets} (Vgl. Abbildung \ref{fig:WirelessKit}) kabellos zu verwenden. Es ist anzumerken, dass für das kabellose Verwenden dieser VR Brille eine \textbf{Erweiterungskarte} in den PC eingebaut werden muss um einen speziellen \textbf{Empfänger} für die Signale der Brille anzuschließen. Des Weiteren muss ein \textbf{Sender} an der VR-Brille angebracht werden, welcher mit dem am PC angeschlossenen Empfänger kommuniziert und durch einen \textbf{mobilen Akku} mit Strom versorgt wird. Der mitgelieferte mobile Akku ermöglicht einen kabellosen Einsatz der VR Brille für bis zu sechs Stunden (Vgl. Abbildung \ref{fig:WirelessKit}) \cite{28}.
\newline\newline
Um die Brille zu verwenden bedarf es mindesten zwei der sogenannten \textbf{SteamVR 2.0 Basisstationen} (Vgl. Abbildung \ref{fig:ViveproKit}), welche dem Bediener in Kombination mit dem Vive Wireless Sets eine enorme Bewegungsfreiheit ermöglichen. Beim Einsatz von zwei solcher Basisstationen ist eine Raumgröße von bis zu 5m x 5m, also 25m² möglich. Es ist sogar möglich bis zu vier solcher Basisstationen zu Verwenden und somit eine Raumgröße von bis zu 10m x 10m, also 100m² zu unterstützen \cite{28}.
\newline\newline
Ein weiteres notwendiges Zubehör der Brille sind die beiden \textbf{Controller} (Vgl. Abbildung \ref{fig:ViveproKit}), die es dem Bediener ermöglichen mit der virtuellen Umgebung zu interagieren. Beide Controller werden wie die VR Brille durch die Basisstationen im Raum geortet und liefern Informationen über ihre eigene Position und Ausrichtung im Raum. Zusätzlich verfügen beide Controller über jeweils fünf Tasten, welche man mit eigenen Funktionalitäten versehen kann. Es ist anzumerken, dass die Tasten alle unterschiedlich sind und daher für unterschiedliche Zwecke verwendet werden können \cite{29}.
\newline
Auf der Vorderseite der Controller befinden sich insgesamt drei Tasten \cite{29}:
\begin{itemize}
	\item Die erste dieser Tasten (ganz unten) ist die Taste um das Hauptmenü aufzurufen. Diese 
	Taste wird in der Regel nicht überschrieben und behält diese Funktionalität bei.
	\item Die zweite Taste (mittig) ist gleichzeitig ein berührungsempfindliches Trackpad. Dem 
	Entwickler steht es frei, ob er diese Taste als einfache Taste oder als Trackpad verwenden 
	möchte. Es ist sogar Möglich beide Funktionen gleichzeitig in einer Anwendung zu 
	unterstützen. Dadurch eröffnen sich viele Anwendungsmöglichkeiten für diese Taste.
	\item Die dritte Taste (oben) ist wiederrum eine ganz einfache Taste und wird in den meisten 
	Anwendungen als eine Menü-Taste verwendet.
\end{itemize}
Auf der Rückseite und der Außenseite des Controllers befinden sich zwei weitere Tasten \cite{29}:
\begin{itemize}
	\item Die Tasten links und rechts an der Außenseite des Controllers bilden eine zusammenhängende Taste, welche ausgelöst wird, wenn der Bediener den Controller fest mit der Hand drückt.
	\item Die Taste auf der Rückseite hat wie das Trackpad auf der Vorderseite zwei 
	Einsatzmöglichkeiten. Sie kann einerseits als einfache Taste verwendet werden, andererseits 
	als berührungsempfindlicher Auslöser, da der Entwickler über die Software-Schnittstelle 
	auslesen kann wie tief die Taste eingedrückt wurde, ähnlich wie bei einem Gaspedal in
	einem Auto.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Bilder/A26_Vivepro}
	\caption{VIVE Pro (mitte), Controller und Basisstationen (außen) \cite{A26}}
	\label{fig:ViveproKit}
\end{figure}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{Bilder/A27_WirelessKit}
	\includegraphics[width=0.4\linewidth]{Bilder/A28_Vive+Wireless}
	\caption{VIVE Wireless Set, inkl. Erweiterungskarte, Sender, Empfänger, Akku, etc. (links) \cite{A27} und VR Brille mit angeschlossenem Sender (rechts) \cite{A28}}
	\label{fig:WirelessKit}
\end{figure}
Neben den außerordentlich guten technischen Spezifikationen der HTC VIVE Pro waren die \textbf{HTC VIVE Tracker} (Vgl. Abbildung \ref{fig:ViveTracker}) ein weiterer Grund warum diese VR Brille zur Umsetzung dieser Arbeit ausgewählt wurde. Die VIVE Tracker werden genauso wie die VR Brille und die dazugehörigen Controller von den Basisstationen im raum geortet und liefern ebenfalls Informationen über ihre Position und Ausrichtung im Raum. Durch den kleinen Formfaktor können die Tracker an beliebigen Objekten befestigt werden um die Bewegung dieser Objekte in der virtuellen Welt abzubilden \cite{30}.
\newline
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\linewidth]{Bilder/A29_ViveTracker}
	\caption{HTC VIVE Tracker \cite{A29}}
	\label{fig:ViveTracker}
\end{figure}
\newline
Bei dieser Arbeit kamen die Tracker für die Ortung der einzelnen Körperteile zum Einsatz, da die Hände und der Kopf bereits mit Hilfe der Controller in den Händen und der VR Brille auf dem Kopf geortet werden können. Konkret kamen die Tracker für die Ortung der Füße, der Knie, des Beckens und der Ellenbogen zum Einsatz. Durch das Schraubgewinde auf der Unterseite lassen sich die Tracker einfach befestigen. Für die Befestigungen am Becken und an den Füßen wurde auf \textbf{fertige Halterungen} zurückgegriffen (Vgl. Abbildung \ref{fig:Mounts}). Um die Tracker an den Knien und an den Ellenbogen zu befestigen habe ich mir \textbf{eigene Halterungen} gebaut (Vgl. Abbildung \ref{fig:Mounts}). Für diese Halterungen wurden handelsübliche Knie- und Ellenbogenschoner verwendet, durch die ein Loch gebohrt wurde um eine Schraube mit Hilfe einer Mutter zu fixieren. Durch das bereits erwähnte Schraubgewinde auf der Unterseite der Tracker ließen diese sich einfach an diesen Schrauben befestigen.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{Bilder/A32_Mounts}
	\caption{Gekaufte (links) und eigene Befestigungen (rechts) für die Tracker, eigene Abbildung}
	\label{fig:Mounts}
\end{figure}

%--------------------------------------------------------------------------------------------------
\section{Eingesetzte Software}\label{sec:Software}
Wie in Abbildung \ref{fig:CodeDarstellung} bereits angedeutet kamen für die Umsetzung dieses Projektes verschiedene Anwendungen und Plugins zum Einsatz. Im Folgenden wird neben den eingesetzten Anwendungen und Plugins insbesondere die verwendete Entwicklungsumgebung Unity Engine genauer erläutert.

\subsection{VIVE Wireless Anwendung}\label{sec:VIVEWireless}
Die VIVE Wireless Anwendung (Vgl. Abbildung \ref{fig:VIVEWirelessSteamVR}) ist für die direkte Verbindung mit der VR Hardware verantwortlich. Wie bereits in Abbildung \ref{fig:WirelessKit} dargestellt wird dafür im Computer eine Erweiterungskarte installiert, die es einem ermöglicht den entsprechenden Empfänger für die Signale am PC anzuschließen. Der in Abbildung \ref{fig:WirelessKit} dargestellte, an der VR Brille montierte Sender, bildet das Gegenstück zu diesem Empfänger. Diese Erweiterung der Hardware, in Kombination mit der VIVE Wireless Anwendung und dem mobilen Akku, ermöglicht die kabellose Verwendung der VR Brille und somit die freie Bewegung des Bedieners im Raum.

\subsection{SteamVR Anwendung}\label{sec:SteamVR}
SteamVR (Vgl. Abbildung \ref{fig:VIVEWirelessSteamVR}) stellt eine weitere wichtige Anwendung im Kontext dieser Arbeit dar. Viele Anwendungen für VR Brillen von unterschiedlichen Herstellern sind auf die Schnittstelle der SteamVR Software ausgelegt. Das Gegenstück zu dieser Schnittstelle bildet das SteamVR Plugin, welches im weiteren Verlauf dieses Kapitels vorgestellt wird und nur für Entwickler relevant ist. Falls die VR Brille ohne das VIVE Wireless Set verwendet wird, wird diese über ein Kabel direkt mit dem PC und somit direkt mit der SteamVR Anwendung verbunden. Für diese Arbeit wurde jedoch das VIVE Wireless Set eingesetzt, somit wurde die VR Brille indirekt über die VIVE Wireless Anwendung mit der SteamVR Anwendung verbunden. Des Weiteren bringt die SteamVR Anwendung eine große Menge an Funktionalitäten mit sich. Dazu gehören Beispielsweise die Möglichkeit den Spielraum zu vermessen oder die Tastenbelegungen der Controller zu verändern. Zusätzlich bietet SteamVR eine Art Hauptmenü an, welches über die in Kapitel \ref{sec:Hardware} angesprochene Taste aufgerufen werden kann. Zusammenfassend lässt sich sagen, dass SteamVR eine zentrale Schnittstelle bietet mit der sich die VR Brille, die Controller, die Basisstationen und die Tracker verwalten lassen.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\linewidth]{Bilder/A33_VIVESteam}
	\caption{Links: VIVE Wireless, Rechts: SteamVR, eigene Abbildung}
	\label{fig:VIVEWirelessSteamVR}
\end{figure}

\subsection{Unity Engine Entwicklungsumgebung}\label{sec:UnitEngine}
Die Unity Engine ist eine 3D-Entwicklungsumgebung und wurde für die Umsetzung dieser Arbeit verwendet. Als Programmiersprache für diese Entwicklungsumgebung kommt die Sprache C\# zum Einsatz. Unity bietet den Vorteil in einem Projekt mehrere Szenen (Umgebungen) aufsetzen und schnell zwischen diesen wechseln zu können. Des Weiteren können diese Szenen direkt in der Anwendung bearbeitet werden, wenn z.B. 3D-Modelle bearbeitet, hinzugefügt oder entfernt werden sollen. Außerdem bringt die Unity Engine eine Vielzahl von bereits eingebauten Funktionalitäten mit sich, die Entwicklern einiges an Arbeit ersparen können. So gibt es Beispielweise vorgefertigte Elemente für graphische Benutzeroberflächen oder sogar ein eingebautes Physiksystem, mit dem sich z.B. Kollisionen von Objekten leicht abfragen können. Dank des eingebauten Asset Stores („Marktplatz“) lassen sich in einem internen Marktplatz aus einer großen Auswahl an Plugins, Erweiterungen, Texturen, Modelle, etc. beliebig viele Komponenten herunterladen und im eigenen Projekt einfügen. Aufgrund dessen kann man sagen, dass Unity als Entwicklungsumgebung den Entwicklern ermöglicht Modularität in Ihren Projekten umzusetzen.
Auch bei dieser Arbeit kamen Erweiterungen aus dem Asset Store zum Einsatz. Neben vereinzelten Texturen aus dem Asset Store sind die wichtigsten Erweiterungen das SteamVR und das Final IK Plugin, welche im weiteren Verlauf dieses Kapitels genauer erläutert werden.
\newline\newline
Bevor die angesprochenen Plugins vorgestellt und die Umsetzung des Menschmodells und der Interaktionsschnittstelle erläutert werden, gibt es zunächst eine Einführung in den Aufbau der Unity Entwicklungsumgebung. Es ist anzumerken, dass das Projekt mit Hilfe der Version 2019.2.19f1 von Unity entwickelt wurde.
\begin{figure}[h]
	\centering
	\includegraphics[width=1\linewidth]{Bilder/A47_UnityOverview}
	\caption{Die Unity Entwicklungsumgebung, eigene Abbildung}
	\label{fig:UnityOverview}
\end{figure}
\newpage
\noindent Das Fenster Entwicklungsumgebung besteht aus den Komponenten Hierarchy (Hierarchie), Scene (Szene), Game (Spiel), Asset Store (Marktplatz), Inspector (Inspektor), Console (Konsole) und Project (Projekt) (Vgl. Abbildung \ref{fig:UnityOverview}).
\begin{enumerate}
	\item \textbf{Hierarchy (Hierarchie)} \\
	Mit Hilfe dieser Komponente enthält der Entwickler einen Überblick über die Objekte in der aktuellen Szene. Dies ermöglicht dem Entwickler Objekte hinzuzufügen, zu bearbeiten oder sogar zu entfernen. Wie in Abbildung \ref{fig:UnityOverview} zu erkennen ist, besteht die Szene der finalen Abgabe dieser Arbeit lediglich aus dem FPS Zähler (Frames per second), der Umgebung (Wände und Boden), den Robotern und dem eigentlichen Menschmodell inklusive der Interaktionsschnittstelle. Eine genauere Erläuterung dieser Komponenten folgt im weiteren Verlauf dieses Kapitels.
	\item \textbf{Scene (Szene)} \\
	Durch die Szenen-Ansicht kann der Entwickler Objekte in der Szene im Raum verschieben, um somit die Anordnung der Objekte zu verändern. Des Weiteren können die Objekte in ihrer Größe verändert und rotiert werden. Die Hauptaufgabe der Szenen-Ansicht liegt darin, dem Entwickler einen Überblick über die gesamte Umgebung zu verschaffen.
	\item \textbf{Game (Spiel)} \\
	In der Spiel-Ansicht enthält der Entwickler beim Ausführen des Programms eine Live-Ansicht der Umgebung. Für diese Arbeit bedeutet das, dass das angezeigte Bild in der Spiel-Ansicht dem in der VR-Brille angezeigten Bild entspricht.
	\item \textbf{Asset Store (Marktplatz)} \\
	Über den Marktplatz können Plugins, Erweiterungen, Texturen, Modelle und sonstige Komponenten von anderen Entwicklern erworben und im eigenen Projekt eingefügt werden. Neben kostenpflichtigen Plugins, wie z.B. dem Final IK Plugin, gibt es dank der großen Entwickler-Community eine enorme Auswahl aus gratis erhältlichen Komponenten.
	\item \textbf{Inspector (Inspektor)} \\
	Der Inspektor erlaubt dem Entwickler schnelle Änderungen vorzunehmen, ohne in den Code eingreifen zu müssen. So kann Beispielsweise die Position, Rotation und Skalierung von Objekten die in der Hierarchie-Ansicht vorher ausgewählt wurden verändert werden.
	\item \textbf{Console (Konsole)} \\
	Mit Hilfe der Konsole enthalten die Entwickler Informationen über Abbrüche oder sonstige Fehler in Form von Textausgaben. Des Weiteren kann die Konsole vom Entwickler während der Entwicklungsphase verwendet werden, um Beispielsweise Textausgaben zu tätigen.
	\item \textbf{Project (Projekt)} \\
	Die Ordnerstruktur des Projekts wird in der Projekt-Ansicht dargestellt. Wie in Abbildung \ref{fig:UnityOverview} zu erkennen ist, befinden sich neben den Scenes (Szenen), Scripts (Code) und Prefabs (Objekte) die bei dieser Arbeit entstanden, auch die Ordner von Plugins wie z.B. SteamVR in der Projekt-Ansicht.
\end{enumerate}

\subsubsection{SteamVR Plugin für die Unity Engine}\label{sec:SteamVRPlugin}
Das SteamVR Plugin bildet wie bereits erwähnt und in Abbildung \ref{fig:CodeDarstellung} illustriert das Gegenstück zur SteamVR Anwendung und vollendet die Schnittstelle zwischen Unity und der VR Hardware. Des Weiteren bietet das Plugin eine große Menge an vorgefertigten Funktionalitäten, wie z.B. das in Abbildung \ref{fig:CodeDarstellung} dargestellte SteamVRTrackedObject Skript.

\subsubsection{Final IK Plugin für die Unity Engine}\label{sec:FinalIKPlugin}
Ein weiteres wichtiges Plugin ist das Final IK Plugin von dem Entwickler RootMotion. Das IK im Namen des Plugins steht für inverse kinematics (deutsch: Inverse Kinematik), also „aus vorhandenen Koordinaten Gelenkwinkel berechnen“ [31, Ham., S.20]. Im Kontext dieser Arbeit bedeutet dies, dass mit Hilfe der gelieferten Koordinaten der Tracker die entsprechenden Gelenkwinkel für die Körperteile berechnet werden. Neben diesen Berechnungen liefert das Final IK Plugin noch das eigentliche Modell des Menschen und einige weitere Funktionalitäten, wie Beispielsweise die Kalibrierung des Modells. Durch die Möglichkeit das Modell zu Kalibrieren und somit an die unterschiedlichen Körpergrößen der verschiedenen Bediener anzupassen, kann für alle Bediener das selbe Modell verwendet werden.

%--------------------------------------------------------------------------------------------------
\section{Das Menschmodell}\label{sec:DasMenschmodell}
In diesem Abschnitt zunächst wird der Aufbau und der Nutzen des Menschmodells ohne die Interaktionsschnittstelle erläutert, da diese beiden Komponenten aufgrund der Anforderungen an die Modularität unabhängig voneinander implementiert wurden.

\subsection{Der Aufbau des Menschmodells}\label{sec:MMAufbau}
Das entstandene Menschmodell basiert auf einem durch das Final IK Plugin gelieferten Modell eines Menschen. Im Folgenden werden die einzelnen Komponenten des entstandenen Modells genauer erläutert. Zu den Komponenten gehören das eigentliche Modell des Menschen, die ergänzenden Komponenten und die Skripte.

\subsubsection{Das eigentliche Modell}\label{sec:MMModell}
Zunächst ist anzumerken, dass das Final IK Plugin mehrere Modelle beinhaltet. Für diese Arbeit wurde das Modell mit dem Namen Dummy ausgewählt, welches wie der Name bereits vermuten lässt wie ein Crashtest-Dummy aussieht (Vgl. Abbildung \ref{fig:Dummy}).
\newline\newline
Das Modell in der Szene trägt den Namen \textbf{Dummy} und enthält das sogenannte \textbf{VR IK Skript} des Final IK Plugins, welches sich um die Animation des Modells kümmert und daher unter anderem Referenzen zu allen Körperteilen enthält. Für die Körperteile Kopf, Hände und Füße gibt es im Inspektor zusätzliche Einstellungsmöglichkeiten (Vgl. Abbildung \ref{fig:TargetBendGoal}). Damit der Dummy animiert werden kann, müssen mindestens der Kopf und die beiden Hände als \textbf{Targets} (Ziele für das Tracking) gesetzt werden. Optional können auch die Füße als Targets gesetzt werden. 
Die Genauigkeit des Trackings kann bei Bedarf verbessert werden, indem die Gelenkpositionen der Knie und Ellenbogen und die Position des Steißbeins als sogenannte \textbf{Bend Goals} (Beug-Ziele) berücksichtigt werden. Falls keine Tracker für die Knie, Ellenbogen und das Steißbein verwendet werden, wird die Position dieser Körperteile anhand der Positionen des Kopfes und der Hände (optional auch anhand der Position der Füße) automatisch durch das VR IK Skript approximiert.
Das zugehörige Bend Goal für den Kopf ist das Steißbein, für die Füße sind es die Knie und für die Hände die Ellenbogen. Zusammenfassend lässt sich sagen, dass es insgesamt fünf solcher Bend Goals gibt, welche im Inspektor gesetzt werden müssen (Vgl. Abbildung \ref{fig:TargetBendGoal}).
Um diesen Prozess zu automatisieren, werden die Targets und die Bend Goals von dem Skript TrackerAssignment, welches im weiteren Verlauf dieses Kapitels vorgestellt wird, verwaltet und gesetzt.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.35\linewidth]{Bilder/A36_TargetsBendGoals}
	\caption{Targets und Bend Goal am Beispiel des linken Arms, eigene Abbildung}
	\label{fig:TargetBendGoal}
\end{figure}
\newline
Des Weiteren enthält das Objekt Dummy mit dem VR IK Skript zwei weitere Kind-Objekte (Vgl. Abbildung \ref{fig:Dummy}), den \textbf{BipDummy} und den gleichnamigen \textbf{Dummy}. Letzterer ist nur der Skinned-Mesh-Renderer und kümmert sich darum, dass die Texturen der Körperteile gerendert werden. Dies bietet den Vorteil, dass man leicht durch neue Texturen das aussehen des Dummy verändern kann. Das Objekt BipDummy enthält die Transformationen, also Position, Rotation und Skalierung aller Körperteile. Dieser Modulare Aufbau ermöglicht es einem das Menschmodell in Zukunft durch beliebige neue Modelle auszutauschen, solange mindestens die Transformationen für die Körperteile Füße, Knie, Steißbein, Hände, Ellenbogen und Kopf vorhanden sind.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.35\linewidth]{Bilder/A34_DummyAufbau}
	\includegraphics[width=0.296\linewidth]{Bilder/A35_Dummy}
	\caption{Final IK Dummy Modell, eigene Abbildung}
	\label{fig:Dummy}
\end{figure}

\subsubsection{Ergänzende Komponenten}\label{sec:MMKomponenten}
Um das Menschmodell einsatzfähig für die VR Hardware zu machen mussten, neben den Skripten, noch zwei weitere Komponenten zu dem eigentlichen Modell hinzugefügt werden (Vgl. Abbildung \ref{fig:UnityOverview}). Diese Komponenten sind das SteamVR CameraRig (Kamera) und die Targets (Ziele) für die optionalen Tracker.
\newline\newline
Das \textbf{SteamVR CameraRig} (Vgl. Abbildung \ref{fig:CameraRig}) ist eine von SteamVR zur Verfügung gestellte Komponente zur Eingrenzung des Spielbereichs und Tracking der Controller (Hier: Hände) und der VR-Brille (Hier: Kopf). Die Komponente besteht aus den drei Kind-Objekten Controller (left), Controler (right) und Camera. Controller (left) und Controller (right) enthalten zusätzlich das 3D-Modell des Controllers als Kind-Objekt, um diesen in der virtuellen Welt anzeigen zu können. Durch diese drei Objekte werden die Bewegungen der beiden Controller und des Kopfes in der virtuellen Welt wiedergespiegelt. Des Weiteren ist es wichtig zu erwähnen, dass das CameraRig für diese Arbeit modifiziert wurde. Den beiden Controllern und der Kamera wurden Kopien der Transformationen der beiden Hände und des Kopfes als Kind-Objekte hinzugefügt. Diese Transformationen kamen beim VR IK Skript (Vgl. Abbildung \ref{fig:TargetBendGoal}) zum Einsatz, um Informationen über die Position und Ausrichtung der Hände und des Kopfes zu erhalten, da diese sich als Kind-Objekte der Controller (Hier: Hände) und der Kamera (Hier: Kopf) entsprechend mitbewegen. Abschließend sei noch zu erwähnen, dass die Transformation vom Kopf (Bip002 Head) um -0,113 Einheiten entlang der Y- und Z-Achse gegenüber dem Parent-Objekt Camera verschoben wurde, damit die Kamera sich nicht innerhalb des Kopfes des Modells, sondern auf Höhe der Augen befindet.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.25\linewidth]{Bilder/A37_CameraRig}
	\caption{Modifiziertes CameraRig, eigene Abbildung}
	\label{fig:CameraRig}
\end{figure}
\newline
Die zweite wichtige Komponente sind die sogenannten \textbf{Targets} (Ziele) (Vgl. Abbildung \ref{fig:Targets}). Da nur sichergestellt ist, dass der Kopf und die Hände durch die VR-Brille und die Controller mit Hilfe des SteamVR CameraRigs verfolgt werden, bedarf es zusätzliche Verfolgungsziele für die optionalen Tracker an den Füßen, Knien, Ellenbogen und dem Steißbein. Die Targets enthalten momentan noch jeweils ein Kind-Objekt (Vgl. Abbildung \ref{fig:Targets}), welches lediglich für den Entwicklungsprozess gedacht ist und auf Wunsch einfach gelöscht werden kann, da es sich nur um eine einfache gefärbte Kugel handelt, die dem Entwickler anzeigt wo der Tracker sich momentan im Raum befinden. Des Weiteren kommen die Targets genauso wie die Transformationen der Hände und des Kopfes im CameraRig beim VR IK Skript zum Einsatz, um dem Skript Informationen über die Position und Ausrichtung der entsprechenden Körperteile zu liefern. Dabei ist anzumerken, dass die Füße als Targets zum Einsatz kommen, während das Steißbein, die Knie und die Ellenbogen als Bend Goals verwendet werden (Vgl. Abbildung \ref{fig:TargetBendGoal}).
\begin{figure}[h]
	\centering
	\includegraphics[width=0.25\linewidth]{Bilder/A38_Targets}
	\caption{Targets, eigene Abbildung}
	\label{fig:Targets}
\end{figure}
\newline
Jedes der angesprochenen Targets enthält, wie bereits in Abbildung \ref{fig:CodeDarstellung} illustriert, das Skript \textbf{SteamVR Tracked Object} (Vgl. Abbildung \ref{fig:TrackedObject}). Durch das bereits erwähnte Skript TrackerAssignment wird jedem der Targets der Index des zugehörigen Trackers zugewiesen. Die Standardeinstellung ist „none“, da das Menschmodell auch ohne jegliche dieser optionalen Tracker und sogar nur mit einer beliebigen Teilmenge von ihnen funktioniert.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.45\linewidth]{Bilder/A39_SteamVRTrackedObject}
	\caption{SteamVR Tracked Object Komponente, eigene Abbildung}
	\label{fig:TrackedObject}
\end{figure}

\subsubsection{Skripte}\label{sec:MMCode}
Für das eigentliche Menschmodell ohne die Interaktionsschnittstelle kommen wie bereits in Abbildung \ref{fig:CodeDarstellung}  und \ref{fig:UnityOverview} dargestellt zwei weitere Skripte zum Einsatz. Diese beiden Skripte tragen die Namen CalibrationController und TrackerAssignment und befinden sich, genauso wie das CameraRig, der Dummy und die Targets, als Kind-Objekte vom Menschmodell (Human) in der Szene.
\newline\newline
Das Skript \textbf{TrackerAssignment} (Vgl. Abbildung \ref{fig:MenschUML}) kümmert sich wie bereits erwähnt um die Zuweisung der Tracker zu den entsprechenden Körperteilen. Aufgrund dessen enthält dieses Skript in öffentlichen Variablen Verweise auf alle SteamVR Tracked Object Skripte der optionalen Tracker, sowie die Transformationen der zugehörigen Körperteile. Des Weiteren enthält dieses Skript in öffentlichen Variablen Verweise auf das Skript CalibrationController und das VR IK Skript vom Dummy. Außerdem enthält dieses Skript noch einige private Variablen, insbesondere die bereits in Kapitel \ref{sec:Variablendefinition} erwähnten IDs der verschiedenen Tracker.
\newline
Insgesamt enthält das Skript zwei Methoden. Die erste Methode trägt den Namen Start und wird wie der Name bereits vermuten lässt einmal bei der Initialisierung ausgeführt und danach nie wieder. In dieser Methode werden lediglich ein paar Variablen die im späteren Verlauf verwendet werden initialisiert. Die zweite Methode trägt den Namen AssignTrackers und beinhaltet die gesamte Funktionalität dieses Skriptes.
In der Methode AssignTrackers werden zunächst mittels einer Schleife alle angeschlossenen Geräte durchlaufen und mit den hinterlegten IDs der Tracker verglichen. Sobald die ID von einem der angeschlossenen Geräte mit einer der hinterlegten IDs übereinstimmt, wird dem SteamVR Tracked Object Skripts des Targets des entsprechenden Körperteils der Index dieses Trackers zugewiesen (Vgl. Abbildung \ref{fig:TrackedObject}). Dies ist notwendig, da die Tracker bei jeder neuen Verbindung mit dem PC einen zufälligen Index zwischen eins und 16 erhalten.
Daraufhin werden dem Calibration Controller die Referenzen zu den Transformationen der Füße und des Steißbeins übergeben (Vgl. Abbildung \ref{fig:CodeDarstellung}), falls die entsprechenden Tracker aktiv sind. In diesem Fall setzt der Calibration Controller automatisch die Füße als Targets und das Steißbein als Bend Goal im VR IK Skript ein. Es ist anzumerken, dass jede beliebige Teilmenge dieser drei Tracker aktiv sein kann, da die Kalibrierung auch nur mit Hilfe der Positionen der Hände und des Kopfes durchgeführt werden kann.
Anschließend werden durch den Verweis auf das VR IK Skript die übrigen Bend Goals zugewiesen, falls die Tracker für die Knie und Ellenbogen angeschlossen sind. Dafür muss zusätzlich das Bend Goal Weight (die Gewichtung des Bend Goals) auf den Wert 1 gesetzt werden (Vgl. Abbildung \ref{fig:TargetBendGoal}). Hier ist ebenfalls anzumerken, dass jede beliebige Teilmenge dieser Tracker aktiv sein kann, ohne das Ergebnis der Darstellung des Menschmodells wesentlich zu verschlechtern. Falls Beispielsweise der Tracker am linken Ellenbogen aktiv ist, aber der Tracker am rechten Ellenbogen nicht, wird die Position des rechten Ellenbogens einfach durch das VR IK Skript anhand der Position und Ausrichtung der rechten Hand und des restlichen Körpers automatisch approximiert.
\newline\newline
Das Skript \textbf{CalibrationController} (Vgl. Abbildung \ref{fig:MenschUML}) arbeitet mit dem bereits vorhandenen VR IK Calibrator des Final IK Plugins. Aufgrund dessen enthält das Skript in öffentlichen Variablen die Verweise auf das VR IK Skript vom Dummy, auf die Transformationen vom Kopf und den beiden Händen und auf das Skript TrackerAssignment. Des Weiteren enthält das Skript die öffentlichen Variablen Settings (Einstellungen) und Data (Daten der Kalibrierung). Schließlich enthält das Skript noch öffentlich deklarierte, aber noch nicht initialisierte Variablen für die Verweise auf die Transformationen des Steißbeins und der beiden Füße.
\newline
Insgesamt enthält das Skript drei Methoden, wobei die letzte den Namen LateUpdate trägt und in der Regel nicht verwendet wird. Diese Methode ist nur für Entwicklungszwecke da und war bereits in einer Demo-Klasse des Final IK Plugins vorhanden. Sie ermöglicht es einem die Kalibrierung des Dummys durch das drücken einer bestimmten Taste auf der Tastatur zu starten. Die erste Methode dieses Skriptes trägt den Namen Update und wird einmal pro Frame ausgeführt. Aufgabe dieser Methode ist es, sobald vom Bediener die entsprechende Taste am linken Controller gedrückt wird, den Kalibrierungsprozess durch Aufruf der zweiten Methode des Skriptes mit dem Namen Calibrate zu starten. Es ist anzumerken, dass der Kalibrierungsprozess beliebig oft ausgeführt und somit die Kalibrierung nach Bedarf erneuert werden kann.
In der Methode Calibrate wird zunächst durch den Verweis auf das Skript TrackerAssignment die Zuweisung der einzelnen Tracker gestartet. Hierbei ist anzumerken, dass wie bereits vorher erwähnt und in Abbildung \ref{fig:CodeDarstellung} dargestellt die noch nicht initialisierten Variablen für die Verweise auf die Transformationen des Steißbeins und der beiden Füße initialisiert werden, falls die entsprechenden Tracker angeschlossen sind. Ansonsten werden für die Kalibrierung lediglich die Positionen der Hände und des Kopfes berücksichtigt. In jedem Fall werden für die Kalibrierung die Positionen der Knie und der Ellenbogen nicht berücksichtigt.
Schließlich wird der eigentliche Kalibrierungsprozess durch die gleichnamige Methode Calibrate des Skripts VR IK Calibrator gestartet, indem die entsprechenden Variablen an diese Methode übergeben werden. Die vorher angesprochenen Variablen Settings und Data sind dafür da, falls man mit Hilfe der Klasse VR IK Calibrator und deren Methoden Kalibrierungen abspeichern und wieder laden möchte. Diese Funktionalität wurde für diese Arbeit nicht berücksichtigt.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\linewidth]{Bilder/A40_MenschUML}
	\caption{UML Diagramm der Klassen TrackerAssignment und CalibrationController, eigene Abbildung}
	\label{fig:MenschUML}
\end{figure}

\subsection{Zusammenfassung des Menschmodells}\label{sec:MMFunktionen}
Das Menschmodell ermöglicht eine Abbildung der Bewegungen des Bedieners auf einen virtuellen Klon. Dabei müssen mindestens die VR Brille und die Controller verwendet werden, um die Positionen der Hände und des Kopfes abbilden zu können. Die Bewegungen der restlichen Körperteile werden in diesem Fall durch das VR IK Skript approximiert. Des Weiteren ermöglicht dieses Menschmodell mit Hilfe der in Abbildung XX dargestellten Befestigungen den Einsatz der HTC VIVE Tracker, um die Bewegungen der Füße, der Knie, der Ellenbogen und des Steißbeins zu berücksichtigen und somit ein genaueres virtuelles Abbild der Bewegungen zu erhalten. Es ist anzumerken, dass nicht jeder dieser Tracker aktiv sein muss und sogar jede beliebige Teilmenge dieser optionalen Tracker aktiv sein kann. Damit wird die in Kapitel XX gestellte Anforderung an die Genauigkeit erfüllt. Dadurch, dass die Abbildung der Bewegungen auf den virtuellen Klon in nahe zu Echtzeit stattfindet, wird die in Kapitel XX gestellte Anforderungen Echtzeit ebenfalls erfüllt. Außerdem ist noch anzumerken, dass durch den Kalibrierungsprozess das Menschmodell durch einen einfachen Knopfdruck an die Körpergröße von dem Bediener angepasst werden kann. Schließlich ist anzumerken, dass aufgrund des Modularen Aufbaus des Menschmodells zukünftige Erweiterungen ermöglicht werden und somit die in Kapitel XX geforderte Modularität erfüllt wird. Da das Modell zusätzlich nicht von seiner virtuellen Umgebung abhängig ist und somit in jeder beliebigen Umgebung eingesetzt werden kann wird auch die Anforderung an die Interoperabilität aus Kapitel XX erfüllt.
%--------------------------------------------------------------------------------------------------
\section{Die Interaktionsschnittstelle}\label{sec:DieInteraktionsschnittstelle}
Da das Menschmodell ohne die Möglichkeit mit der Umgebung zu interagieren nur bedingt nützlich ist, wird in diesem Abschnitt der Aufbau und der Nutzen der Interaktionsschnittstelle erläutert. Die Grundidee der Interaktionsschnittstelle ist es das Menschmodell durch einen Pointer (Zeiger) zu erweitern und somit eine intuitive Interaktion mit der Umgebung zu ermöglichen (Vgl. Abbildung XX).
\begin{figure}[h]
	\centering
	\includegraphics[width=0.65\linewidth]{Bilder/A44_InteraktionsBeispiel}
	\caption{Grundidee der Interaktion, eigene Abbildung}
	\label{fig:InteraktionBeispiel}
\end{figure}

\subsection{Der Aufbau der Interaktionsschnittstelle}\label{sec:AufbauInteraktion}
Im Folgenden zunächst das Grundgerüst der Interaktionsschnittstelle erklärt, bevor die zusätzlichen Funktionalitäten erläutert werden.

\subsubsection{Das Grundgerüst der Interaktionsschnittstelle}\label{sec:GrundgerüstInteraktion}
Das Grundgerüst der Interaktionsschnittstelle besteht lediglich aus dem Interaktionssystem, den Pointern, der Klasse die den Wechsel zwischen den beiden Pointern durchführt und der Klasse die das Menü des angeklickten Objektes aufruft. Es wurden zwei Pointer verwendet, da der eine Pointer genutzt wird um mit Objekten wie z.B. Robotern in der Umgebung zu interagieren, während der andere Pointer dafür da ist um mit Menüs zu interagieren. Es ist anzumerken, dass die Klassen VR Input für das Interaktionssystem und VR Canvas Pointer und VR Physics Pointer für die beiden Pointer auf Anleitungen des Youtube Kanals "VR with Andrew" basieren [32, VRWithAndrew].
\newline\newline
Das \textbf{Interaktionssystem} (Vgl. Abbildung XX) basiert auf dem sogenannten Event System von Unity, welches standardmäßig in jeder Szene vorhanden ist. Mit Hilfe des Event Systems werden in der Regel der Input der Tastatur und der Maus gehandhabt. Das Event System wird für diese Arbeit durch die Klasse \textbf{VR Input} erweitert. Die Klasse enthält die drei öffentlichen Variablen eventCamera, clickButton und clickAction. Die Variable eventCamera erhält einen Verweis auf die Kamera des aktuell genutzten Pointers, während die Variablen clickButton und clickAction lediglich genutzt werden um Input-Taste zu deklarieren.
\newline
Insgesamt enthält die Klasse fünf Methoden. Die erste Methode trägt den Namen Awake, dient lediglich zur Initialisierung und wird automatisch ausgeführt sobald eine der Methoden in der Klasse ausgeführt wird. Die nächsten vier Methoden sind dafür da um den Input der Maus zu überschreiben. Die erste dieser vier Methoden trägt den Namen GetMouseButton und liefert Informationen darüber ob Taste gedrückt und wieder losgelassen wurde. Die Methoden GetMouseButtonDown und GetMouseButtonUp ermöglichen es dem Entwickler separat abzufragen, ob die Taste runtergerückt oder losgelassen wurde. Somit wird durch das Klicken der entsprechenden Taste am rechten Controller das Klicken mit der Maus simuliert. Die letzte Methode trägt den Namen mousePosition und überschreibt die Position der Maus durch den exakten Mittelpunkt des Anzeigebildschirms.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{Bilder/A41_EventSystem}
	\caption{Event System mit VR Input Aufbau in der Szene, eigene Abbildung}
	\label{fig:EventSystem}
\end{figure}
\newline
Der aktuell eingesetzte Pointer in wird sobald das Programm ausgeführt wird als Kind-Objekt des rechten Controllers (Vgl. Abbildung XX) instanziiert. Somit bewegt sich der Pointer mit der rechten Hand bzw. mit dem rechten Controller mit. Zu Beginn wird Standardmäßig der Physics Pointer instanziiert, sobald jedoch ein Objekt angeklickt wird und sich somit das Menü dieses Objektes öffnet, wird auf den Canvas Pointer gewechselt.
\newline
Sowohl der \textbf{Canvas Pointer} als auch der \textbf{Physics Pointer} enthalten die Komponenten Camera, Line Renderer und das zugehörige Skript. Der Physics Pointer enthält zusätzlich die Komponente Physics Raycaster (Vgl. Abbildung XX). Mit Hilfe der Komponente Camera lässt sich bestimmen worauf der aktuelle Pointer zeigt. Diese Komponente wird durch das im späteren Verlauf dieses Kapitels vorgestellte Skript Switch Pointers beim Event System hinterlegt (Vgl. Abbildung XX). Der Line Renderer hat lediglich die Aufgabe eine Linie zu zeichnen, damit der Bediener erkennen kann worauf er klickt. Diese Linie beginnt beim Controller in der rechten Hand des Bedieners und ist standardmäßig drei Unity-Einheiten lang. Falls ein Objekt die Linie schneidet, endet diese an dem Schnittpunkt. 
\newline
Grundsätzlich funktionieren die Klassen VR Physics Pointer und VR Canvas Pointer nach dem gleichen Prinzip. Es wird ausgehend vom Ursprung des Pointers ein Raycast („Strahl“) gesendet und überprüft womit dieser interagiert. Der unterschied ist, dass durch die Klasse VR Physics Pointer ein Raycast gesendet wird, der nur mit Objekten die einen Collidern besitzen interagiert (Physics Raycast). Die Klasse VR Canvas Pointer hingegen sendet mit Hilfe des Event Systems einen Raycast, der nur mit Objekten die Bestandteil einer graphischen Benutzeroberfläche sind interagiert. Daher ist die einzige Anforderung an die Umgebung, dass die Objekte mit denen interagiert werden soll einen Collider besitzen und das die Menüs die bereits in Unity vorhandenen Elemente wie z.B. Knöpfe oder Slider für grafische Benutzeroberflächen nutzen.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{Bilder/A42_CanvasPointer}
	\includegraphics[width=0.5\linewidth]{Bilder/A43_PhysicsPointer}
	\caption{Canvas Pointer und Physics Pointer Aufbau in der Szene, eigene Abbildung}
	\label{fig:Pointer}
\end{figure}
\newline
Der Wechsel zwischen den Pointern wird durch die Klasse \textbf{Switch Pointers} gehandhabt. Diese enthält in öffentlichen Variablen Verweise auf die Objektvorlagen der beiden Pointer (Vgl. Abbildung XX).
\newline
Zu Beginn wird in der Methode Start wie bereits erwähnt ein Physics Pointer instanziiert, dessen Kamera Komponente als Event Camera beim Event System gesetzt wird (Vgl. Abbildung XX). Die Methode Update ruft die Methode HandlePointerSwitch jede Sekunde mehrfach auf. Sobald ein Objekt angeklickt wird und sich sein Menü öffnet, wird der Pointer mit Hilfe der Methode HandlePointerSwitch ausgetauscht und die Event Camera beim Event System aktualisiert. Des Weiteren werden bei dem Canvas Pointer die Referenzen zum Event System und seinem Input Module gesetzt (Vgl. Abbildung XX). Die Beiden Hilfsmethoden SpawnPhysicsPointer und SpawnCanvasPointer kommen zum Einsatz um den entsprechenden Pointer zu instanziieren.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{Bilder/A45_SwitchPointer}
	\caption{Switch Pointers Aufbau in der Szene, eigene Abbildung}
	\label{fig:SwitchPointer}
\end{figure}
\newline
Die letzte Komponente des Grundgerüsts der Interaktionsschnittstelle ist die Klasse \textbf{Object Menu}. Diese Klasse kann zu jedem beliebigen Objekt in der Szene hinzugefügt werden. Die wichtigsten Komponenten dieser Klasse sind die öffentlichen Variablen prefab, thisObject, clickButton und clickAction (Vgl. Abbildung XX). Die Variable prefab enthält den Verweis auf die Objektvorlage des zu öffnenden Menüs und die variable thisObject enthält den Objektverweis auf das Objekt zu dem das Skript gehört. Die Variablen clickButton und clickAction werden lediglich genutzt um die Input-Taste für das Schließen des Menüs zu deklarieren.
\newline
Insgesamt enthält diese Klasse drei Methoden. In der Methode Start werden lediglich ein paar private Hilfsvariablen deklariert und in der Methode Update wird, falls das Objekt angeklickt wird die Methode HandleButtonPress aufgerufen. Diese Methode instanziiert dann eine Instanz des zu öffnenden Menüs. Beim erneuten aufrufen der Methode HandleButtonPress durch das Drücken der entsprechenden Taste am rechten Controller wird das bereits geöffnete Menü wieder geschlossen. Des Weiteren aktualisiert die Methode Update die Position des Menüs mehrmals pro Sekunde, sodass sich das Menü mit der Kopfbewegung des Bediener mitbewegt und stehts gut sichtbar ist.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{Bilder/A46_ObjectMenu}
	\caption{Object Menu Aufbau in der Szene, eigene Abbildung}
	\label{fig:ObjectMenu}
\end{figure}
\newline
--> TODO UML

\subsubsection{Weitere Bestandteile der Interaktionsschnittstelle}\label{sec:WeitereTeileInteraktion}
Basierend auf dem im vorherigen Abschnitt erläuterten Grundgerüst werden in diesem Abschnitt zusätzliche Bestandteile der bei dieser Arbeit entstandenen Interaktionsschnittstelle erläutert. In Zukunft könnten die folgenden Bestandteile nach Bedarf ausgetauscht oder verbessert werden, daher wird der Aufbau des Codes der einzelnen Bestandteile im Folgenden nicht im Detail erläutert. Mit Hilfe dieser zusätzlichen Bestandteile soll ein Anwendungsbeispiel für die Interaktionsschnittstelle geschaffen werden, um somit den Nutzen zu demonstrieren.
\newline\newline
Um den Bediener ein optisches Feedback zu geben und somit die Interaktion intuitiver zu gestalten können bei Bedarf die Skripte PointerEvent\_Objects für Objekte und PointerEvent\_UI für Objekte die Bestandteil der graphischen Benutzeroberfläche sind verwendet werden. Dafür müssen diese Skripte lediglich zu den gewünschten Objekten hinzugefügt werden. Mit Hilfe dieser Skripte verändern Objekte auf die mit dem Pointer gezeigt wird ihre Farbe (Vgl. Abbildung XX). Objekte auf die lediglich gezeigt wird werden grau gefärbt, sodass der Bediener schnell und intuitiv erkennen kann worauf er gerade zeigt, während angeklickte Objekte rot gefärbt werden, um dem Bediener zu signalisieren, dass das Objekt erfolgreich angeklickt wurde. An dieser Stelle ist anzumerken, dass die Idee diese Funktionalität inspiriert wurde durch den YouTube Kanal VR with Andres [32, VRWithAndrew].
\newline TODO Bilder
\newline\newline
Ein weiterer Bestandteil der Interaktionsschnittstelle ist das Hauptmenü, welches durch das Klicken der obersten Taste am linken Controller durch das Skript Player Menu aufgerufen wird. Das Hauptmenü wurde hinzugefügt um einen Anwendungsfall der Interaktionsschnittstelle zu demonstrieren und somit den Nutzen hervorzuheben. Insgesamt besteht das Hauptmenü aus drei Benutzeroberflächen. Die erste Benutzeroberfläche ist die Startseite des Hauptmenüs und zeigt dem Bediener den aktuellen Batteriestatus der benutzten Hardware an. Auf die zweite Benutzeroberfläche des Hauptmenüs gelangt man durch das anklicken des Knopfes „Options“. Hier können durch den Entwickler Einstellungsmöglichkeiten wie z.B. die maximale Anzahl bestimmter Roboter in einer Produktionsanlage festgelegt werden. Die letzte Benutzeroberfläche ist das Inventar und wird durch das Anklicken des Knopfes „Inventory“ aufgerufen. Sie bietet dem Entwickler die Möglichkeit ein Inventar zu implementieren, über welches Produktionsanlagen in den Produktionsräumen platziert werden können. Für diese Arbeit wurden zu Demonstrationszwecken vier Roboter stellvertretend für Elemente einer Produktionsanlage hinzugefügt, welche über das Inventar-Menü in der Umgebung platziert werden können.
\newline TODO Bilder
\newline\newline
Um das Hinzufügen von Robotern in der Szene zu ermöglichen gibt es noch die Skripte Global Variables und Spawning Handler. Durch das Skript Spawning Handler werden Roboter instanziiert und in der Szene platziert, sobald dies über das Inventar-Menü durch den Bediener aufgetragen wird. Global Variables hat die Funktion die aktuelle und maximale Anzahl der Roboter zu verwalten. Des Weiteren verfügt diese Klasse über Referenzen zu den Objektdateien aller vorhandenen Roboter. Damit Beispielsweise nicht mehr Roboter von einem Typ als vorher im Optionen-Menü festgelegt in der Szene platziert werden können, nutzen Skripte wie Spawning Handler oder auch Player Menu das Skript Global Variables. Somit wird sichergestellt, dass wichtige Variablen zentral und für alle Klassen erreichbar verwaltet werden.
\newline TODO Bilder
\newline\newline
Die bereits angesprochenen vier Roboter die zu demonstrationszwecken in das Projekt eingefügt wurden verfügen alle über ihre eigenen Menüs. Die Menüs bestehen insgesamt aus zwei Benutzeroberflächen. Die erste Benutzeroberfläche kann mit anwendungsspezifischen Funktionalitäten gefüllt werden, während die zweite Benutzeroberfläche bei allen vier Robotern gleich aufgebaut ist. Mit Hilfe der vorhandenen Knöpfe kann der entsprechende Roboter in der virtuellen Produktionsanlage nach Bedarf verschoben oder sogar wieder in das Inventar aufgenommen werden. Es ist anzumerken, dass die Menüs der einzelnen Roboter unabhängig von der Interaktionsschnittstelle sind und daher beliebig viele Benutzeroberflächen beinhalten können. Selbst das Aufnehmen eines Roboters in das Inventar bedarf lediglich den Aufruf einer Methode der Klasse Spawning Handler.
\newline TODO Bilder